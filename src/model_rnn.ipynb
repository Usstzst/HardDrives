{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c13130-70ef-499f-a82e-a62b66fa38e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julianna\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52de5aac-962d-44eb-8ac8-cead46abbeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ZST\\HardDrives\\src\\model\\RNN.py:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  from pandas import np\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975b691-3180-403d-8a5b-b52500cb2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_predict(model, train_loader,test_loader):\n",
    "    ## Train\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([1, 1]))\n",
    "    # criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    losses = {'train': [], 'test': []}\n",
    "    for epoch_idx in range(120):\n",
    "        train_loss = rnn_train_epoch(model, train_loader, optimizer, criterion)\n",
    "        test_loss = evaluate(model, test_loader, criterion)\n",
    "        losses['train'].append(train_loss)\n",
    "        losses['test'].append(test_loss)\n",
    "        # print(epoch_idx, train_loss, test_loss['loss'], test_loss['FAR'], test_loss['FDR'])\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77c332d-2914-46cb-8e8a-3a495b4e6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from utils import FAR, FDR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec09b91-697d-463f-8082-1318f9bd0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [[3, 0, 0, 1, 1, 2],\n",
    "             [3, 0, 1, 2],\n",
    "             [3, 0, 0, 0, 1, 1, 1, 2],\n",
    "             [3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e615fa4-5f05-4122-9bcf-3a547892dfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d5d57d-5ec6-4982-b3cc-465c30f6e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(train_set):\n",
    "    # print('seq:', seq)\n",
    "    loss = 0\n",
    "    for t in range(len(seq) - 1):\n",
    "        x = torch.Tensor([seq[t]])\n",
    "        y = torch.Tensor([seq[t +1]])\n",
    "        # print('x:', x)\n",
    "        # print('y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e3f4e1-7f20-4526-b082-4941de0f44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2.])\n",
      "tensor([3.])\n",
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "from model.RNN import RNN\n",
    "model = RNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_set = [[3, 0, 0, 2, 1, 0],\n",
    "             [3, 0, 1, 1, 2, 1],\n",
    "             [3, 0, 1, 1, 2, 1],\n",
    "             [3, 1, 1, 1, 1, 1]]\n",
    "\n",
    "# 重复进行50次试验\n",
    "num_epoch = 5\n",
    "loss_list = []\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss = 0\n",
    "    # 对train_set中的数据进行随机洗牌，以保证每个epoch得到的训练顺序都不一样。\n",
    "    np.random.shuffle(train_set)\n",
    "    # 对train_set中的数据进行循环\n",
    "    for i, seq in enumerate(train_set):\n",
    "        loss = 0\n",
    "        for t in range(len(seq) - 1):\n",
    "\n",
    "            x = torch.Tensor([seq[t]])\n",
    "            y = torch.Tensor([seq[ t +1]])\n",
    "            # print(x)\n",
    "            \n",
    "            output, h_n = model(x)  # 综合加了RNN的模型输出 输出\n",
    "            loss += criterion(output ,y.long())\n",
    "\n",
    "        loss = 1.0 * loss / len(seq)  # 计算每字符的损失数值\n",
    "        # print(loss)\n",
    "        optimizer.zero_grad() # 梯度清空\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 一步梯度下降"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
